{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e9e5a9",
   "metadata": {},
   "source": [
    "\n",
    "# Universal Image Crawler and Preprocessor\n",
    "\n",
    "This Jupyter notebook helps you:\n",
    "- Crawl any images from Google Images using `icrawler`\n",
    "- Manually inspect and delete irrelevant images\n",
    "- Automatically rename all remaining images\n",
    "- Filter and resize images for YOLO training\n",
    "\n",
    "## üõ†Ô∏è Required Packages\n",
    "\n",
    "Please install the following packages before running the notebook:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install icrawler opencv-python tqdm pillow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Setup bread types and create base/category directories\n",
    "\n",
    "import os\n",
    "\n",
    "bread_types = [\"croissant\", \"Îã®Ìå•Îπµ\", \"ÏÜåÎ≥¥Î°ú Îπµ\"]  # You can modify this list\n",
    "base_dir = \"raw_images_crawled\"\n",
    "MAX_IMAGES = 200 \n",
    "\n",
    "# Create base directory if it doesn't exist\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "    print(f\"Created base directory: {base_dir}\")\n",
    "else:\n",
    "    print(f\"Base directory already exists: {base_dir}\")\n",
    "\n",
    "# Create subdirectories for each bread type\n",
    "for bread in bread_types:\n",
    "    safe_name = bread.replace(\" \", \"_\")\n",
    "    image_dir = os.path.join(base_dir, safe_name, \"images\")\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    print(f\"Prepared directory for '{bread}': {image_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Image crawling with icrawler\n",
    "\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "\n",
    "for bread in bread_types:\n",
    "    keyword = f\"{bread} bread\"\n",
    "    safe_name = bread.replace(\" \", \"_\")\n",
    "    save_path = os.path.join(base_dir, safe_name, \"images\")\n",
    "    print(f\"Starting image crawl for: {bread} (target: {MAX_IMAGES})\")\n",
    "\n",
    "    crawler = GoogleImageCrawler(storage={\"root_dir\": save_path})\n",
    "    crawler.crawl(keyword=keyword, max_num=MAX_IMAGES)\n",
    "\n",
    "print(\"Image crawling completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e167c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2.5: Rename all remaining image files to 0000.jpg, 0001.jpg, ... (after manual filtering)\n",
    "\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "for bread in bread_types:\n",
    "    safe_name = bread.replace(\" \", \"_\")\n",
    "    image_dir = os.path.join(base_dir, safe_name, \"images\")\n",
    "\n",
    "    image_paths = sorted(\n",
    "        glob(os.path.join(image_dir, \"*\")),\n",
    "        key=lambda x: os.path.getmtime(x)\n",
    "    )\n",
    "\n",
    "    print(f\"Renaming images for category: {bread} ({len(image_paths)} files)\")\n",
    "\n",
    "    for idx, path in enumerate(image_paths):\n",
    "        new_path = os.path.join(image_dir, f\"{idx:04d}.jpg\")\n",
    "        try:\n",
    "            img = cv2.imread(path)\n",
    "            if img is not None:\n",
    "                cv2.imwrite(new_path, img)\n",
    "            if path != new_path:\n",
    "                os.remove(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Preprocessing functions - filter and resize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def is_blurry(image, threshold=100.0):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() < threshold\n",
    "\n",
    "def process_image(path, output_path, size=(640, 640)):\n",
    "    try:\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            return False\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if h < 300 or w < 300:\n",
    "            return False\n",
    "        if h < size[0] or w < size[1]:\n",
    "            return False\n",
    "        if is_blurry(image):\n",
    "            return False\n",
    "\n",
    "        resized = cv2.resize(image, size)\n",
    "        cv2.imwrite(output_path, resized)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Apply preprocessing to all crawled images\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for bread in bread_types:\n",
    "    safe_name = bread.replace(\" \", \"_\")\n",
    "    image_dir = os.path.join(base_dir, safe_name, \"images\")\n",
    "    image_paths = glob(os.path.join(image_dir, \"*\"))\n",
    "\n",
    "    print(f\"Processing images for: {bread} ({len(image_paths)} files)\")\n",
    "\n",
    "    for idx, path in enumerate(tqdm(image_paths)):\n",
    "        output_path = os.path.join(image_dir, f\"{idx:04d}.jpg\")\n",
    "        success = process_image(path, output_path)\n",
    "        if not success and os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "\n",
    "print(\"All image preprocessing completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
