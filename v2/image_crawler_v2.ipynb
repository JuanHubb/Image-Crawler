{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Read keywords from external file\n",
    "def read_keywords(filename=\"keywords.txt\"):\n",
    "    \"\"\"Read keywords from a text file, one keyword per line\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Error: {filename} not found. Creating sample file...\")\n",
    "        # Create a sample keywords file\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"croissant\\n\")\n",
    "            f.write(\"단팥빵\\n\")\n",
    "            f.write(\"소보로 빵\\n\")\n",
    "        print(f\"Sample {filename} created. Please edit it with your desired keywords.\")\n",
    "        return []\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        keywords = [line.strip() for line in f if line.strip()]\n",
    "    return keywords\n",
    "\n",
    "# Read keywords from file\n",
    "bread_types = read_keywords(\"keywords.txt\")\n",
    "base_dir = \"raw_images_crawled\"\n",
    "TARGET_IMAGES = 200  # Target number of images per keyword\n",
    "MAX_IMAGES_PER_SEARCH = 150  # Images to request per individual search\n",
    "\n",
    "print(f\"Loaded {len(bread_types)} keywords: {bread_types}\")\n",
    "\n",
    "# Create base directory if it doesn't exist\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "    print(f\"Created base directory: {base_dir}\")\n",
    "else:\n",
    "    print(f\"Base directory already exists: {base_dir}\")\n",
    "\n",
    "# Create subdirectories for each bread type\n",
    "for bread in bread_types:\n",
    "    safe_name = bread.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    image_dir = os.path.join(base_dir, safe_name, \"images\")\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "    print(f\"Prepared directory for '{bread}': {image_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Simple image crawling (based on working first version)\n",
    "\n",
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Configuration options\n",
    "USE_KEYWORD_VARIATIONS = False  # Set to False to use only original keywords\n",
    "ADD_BREAD_SUFFIX = True         # Set to True to add \"bread\" to keywords (like original)\n",
    "SKIP_CRAWLING = False          # Set to True to skip crawling entirely\n",
    "\n",
    "def simple_crawl(keyword, save_path, max_images):\n",
    "    \"\"\"Simple crawling function similar to the original working version\"\"\"\n",
    "    try:\n",
    "        print(f\"Starting image crawl for: {keyword} (target: {max_images})\")\n",
    "        \n",
    "        # Use simple settings like the original\n",
    "        crawler = GoogleImageCrawler(storage={\"root_dir\": save_path})\n",
    "        crawler.crawl(keyword=keyword, max_num=max_images)\n",
    "        \n",
    "        # Count downloaded images\n",
    "        import os\n",
    "        if os.path.exists(save_path):\n",
    "            count = len([f for f in os.listdir(save_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp'))])\n",
    "            print(f\"Downloaded {count} images for '{keyword}'\")\n",
    "            return count\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during crawling with keyword '{keyword}': {e}\")\n",
    "        return 0\n",
    "\n",
    "def enhanced_crawl(base_keyword, save_path, target_count):\n",
    "    \"\"\"Enhanced version with multiple keywords but simple approach\"\"\"\n",
    "    \n",
    "    # Generate keyword list\n",
    "    if USE_KEYWORD_VARIATIONS:\n",
    "        keywords = [\n",
    "            base_keyword,\n",
    "            f\"{base_keyword} bread\",\n",
    "            f\"{base_keyword} bakery\",\n",
    "            f\"fresh {base_keyword}\",\n",
    "            f\"{base_keyword} food\"\n",
    "        ]\n",
    "    else:\n",
    "        if ADD_BREAD_SUFFIX:\n",
    "            keywords = [f\"{base_keyword} bread\"]  # Like original working version\n",
    "        else:\n",
    "            keywords = [base_keyword]\n",
    "    \n",
    "    total_downloaded = 0\n",
    "    \n",
    "    for i, keyword in enumerate(keywords):\n",
    "        if total_downloaded >= target_count:\n",
    "            break\n",
    "        \n",
    "        remaining = target_count - total_downloaded\n",
    "        max_for_this_search = min(remaining + 20, MAX_IMAGES_PER_SEARCH)\n",
    "        \n",
    "        print(f\"\\nSearch {i+1}/{len(keywords)}: '{keyword}'\")\n",
    "        \n",
    "        # Simple crawl like the original\n",
    "        count = simple_crawl(keyword, save_path, max_for_this_search)\n",
    "        total_downloaded = count  # Update with actual count\n",
    "        \n",
    "        # Small delay between searches\n",
    "        if i < len(keywords) - 1:  # Not the last keyword\n",
    "            delay = random.uniform(2, 4)\n",
    "            print(f\"Waiting {delay:.1f} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return total_downloaded\n",
    "\n",
    "# Main crawling loop\n",
    "if not SKIP_CRAWLING:\n",
    "    print(\"Configuration:\")\n",
    "    print(f\"- Keyword variations: {'Enabled' if USE_KEYWORD_VARIATIONS else 'Disabled'}\")\n",
    "    print(f\"- Add 'bread' suffix: {'Enabled' if ADD_BREAD_SUFFIX else 'Disabled'}\")\n",
    "    print()\n",
    "    \n",
    "    for bread in bread_types:\n",
    "        safe_name = bread.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        save_path = os.path.join(base_dir, safe_name, \"images\")\n",
    "        \n",
    "        print(f\"Processing: {bread}\")\n",
    "        print(f\"Target: {TARGET_IMAGES} images\")\n",
    "        \n",
    "        if USE_KEYWORD_VARIATIONS or not ADD_BREAD_SUFFIX:\n",
    "            # Use enhanced crawl\n",
    "            final_count = enhanced_crawl(bread, save_path, TARGET_IMAGES)\n",
    "        else:\n",
    "            # Use simple crawl (exactly like original)\n",
    "            keyword = f\"{bread} bread\" if ADD_BREAD_SUFFIX else bread\n",
    "            final_count = simple_crawl(keyword, save_path, TARGET_IMAGES)\n",
    "        \n",
    "        print(f\"Completed: '{bread}' - {final_count} images downloaded\")\n",
    "        \n",
    "        # Delay between different bread types\n",
    "        if bread != bread_types[-1]:  # Not the last item\n",
    "            delay = random.uniform(3, 6)\n",
    "            print(f\"Waiting {delay:.1f} seconds before next category...\\n\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "else:\n",
    "    print(\"Crawling skipped.\")\n",
    "    \n",
    "    # Show existing counts\n",
    "    for bread in bread_types:\n",
    "        safe_name = bread.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        image_dir = os.path.join(base_dir, safe_name, \"images\")\n",
    "        \n",
    "        if os.path.exists(image_dir):\n",
    "            count = len([f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp'))])\n",
    "            print(f\"Existing images for '{bread}': {count}\")\n",
    "\n",
    "print(\"\\nImage crawling completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Rename all remaining image files to 0000.jpg, 0001.jpg, etc.\n",
    "# Run this after manual filtering of irrelevant images\n",
    "\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "def rename_images_in_directory(image_dir, bread_name):\n",
    "    \"\"\"Rename all images in directory to sequential format\"\"\"\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.gif', '*.bmp', '*.webp']\n",
    "    image_paths = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob(os.path.join(image_dir, ext)))\n",
    "        image_paths.extend(glob(os.path.join(image_dir, ext.upper())))\n",
    "    \n",
    "    # Sort by modification time\n",
    "    image_paths = sorted(image_paths, key=lambda x: os.path.getmtime(x))\n",
    "    \n",
    "    print(f\"Renaming images for category: {bread_name} ({len(image_paths)} files)\")\n",
    "    \n",
    "    # Create temporary directory to avoid naming conflicts\n",
    "    temp_dir = os.path.join(image_dir, \"temp_rename\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    successfully_renamed = 0\n",
    "    \n",
    "    # First, move all files to temp directory with new names\n",
    "    for idx, path in enumerate(image_paths):\n",
    "        temp_path = os.path.join(temp_dir, f\"{idx:04d}.jpg\")\n",
    "        try:\n",
    "            img = cv2.imread(path)\n",
    "            if img is not None:\n",
    "                cv2.imwrite(temp_path, img)\n",
    "                successfully_renamed += 1\n",
    "            else:\n",
    "                print(f\"Warning: Could not read image {os.path.basename(path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(path)}: {e}\")\n",
    "    \n",
    "    # Remove original files\n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing {os.path.basename(path)}: {e}\")\n",
    "    \n",
    "    # Move renamed files back to original directory\n",
    "    temp_files = glob(os.path.join(temp_dir, \"*.jpg\"))\n",
    "    for temp_file in temp_files:\n",
    "        final_path = os.path.join(image_dir, os.path.basename(temp_file))\n",
    "        try:\n",
    "            os.rename(temp_file, final_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error moving {os.path.basename(temp_file)}: {e}\")\n",
    "    \n",
    "    # Remove temporary directory\n",
    "    try:\n",
    "        os.rmdir(temp_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(f\"Successfully renamed {successfully_renamed} images for {bread_name}\")\n",
    "    return successfully_renamed\n",
    "\n",
    "# Rename images for all bread types\n",
    "for bread in bread_types:\n",
    "    safe_name = bread.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    image_dir = os.path.join(base_dir, safe_name, \"images\")\n",
    "    \n",
    "    if os.path.exists(image_dir):\n",
    "        rename_images_in_directory(image_dir, bread)\n",
    "    else:\n",
    "        print(f\"Warning: Directory not found for {bread}: {image_dir}\")\n",
    "\n",
    "print(\"\\nImage renaming completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Preprocessing functions - filter and resize images\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def is_blurry(image, threshold=100.0):\n",
    "    \"\"\"Check if image is blurry using Laplacian variance\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var() < threshold\n",
    "\n",
    "def is_too_dark_or_bright(image, dark_threshold=30, bright_threshold=220):\n",
    "    \"\"\"Check if image is too dark or too bright\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    mean_brightness = np.mean(gray)\n",
    "    return mean_brightness < dark_threshold or mean_brightness > bright_threshold\n",
    "\n",
    "def has_sufficient_content(image, min_std=15):\n",
    "    \"\"\"Check if image has sufficient content variation\"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.std(gray) > min_std\n",
    "\n",
    "def process_image(input_path, output_path, target_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Process a single image: validate, filter, and resize\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input image\n",
    "        output_path: Path to save processed image\n",
    "        target_size: Target size tuple (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if image was successfully processed, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read image\n",
    "        image = cv2.imread(input_path)\n",
    "        if image is None:\n",
    "            return False\n",
    "        \n",
    "        # Get original dimensions\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Filter out images that are too small\n",
    "        if h < 300 or w < 300:\n",
    "            return False\n",
    "        \n",
    "        # Filter out images that would need significant upscaling\n",
    "        if h < target_size[0] * 0.75 or w < target_size[1] * 0.75:\n",
    "            return False\n",
    "        \n",
    "        # Apply quality filters\n",
    "        if is_blurry(image):\n",
    "            return False\n",
    "        \n",
    "        if is_too_dark_or_bright(image):\n",
    "            return False\n",
    "        \n",
    "        if not has_sufficient_content(image):\n",
    "            return False\n",
    "        \n",
    "        # Resize image maintaining aspect ratio\n",
    "        # Calculate scaling factor\n",
    "        scale_w = target_size[0] / w\n",
    "        scale_h = target_size[1] / h\n",
    "        scale = min(scale_w, scale_h)\n",
    "        \n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        \n",
    "        # Resize image\n",
    "        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Create canvas with target size and paste resized image in center\n",
    "        canvas = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)\n",
    "        canvas.fill(114)  # Fill with gray color (common in YOLO)\n",
    "        \n",
    "        # Calculate position to center the image\n",
    "        y_offset = (target_size[1] - new_h) // 2\n",
    "        x_offset = (target_size[0] - new_w) // 2\n",
    "        \n",
    "        canvas[y_offset:y_offset + new_h, x_offset:x_offset + new_w] = resized\n",
    "        \n",
    "        # Save processed image\n",
    "        cv2.imwrite(output_path, canvas)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(input_path)}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"Preprocessing functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Apply preprocessing to all crawled images\n",
    "\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "def preprocess_category_images(bread_name, image_dir, target_size=(640, 640)):\n",
    "    \"\"\"Preprocess all images in a category directory\"\"\"\n",
    "    \n",
    "    # Get all jpg files (should be renamed by previous step)\n",
    "    image_paths = sorted(glob(os.path.join(image_dir, \"*.jpg\")))\n",
    "    \n",
    "    if not image_paths:\n",
    "        print(f\"No images found for {bread_name}\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Processing images for: {bread_name} ({len(image_paths)} files)\")\n",
    "    \n",
    "    # Create processed directory\n",
    "    processed_dir = os.path.join(os.path.dirname(image_dir), \"processed\")\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    successful_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    # Process each image\n",
    "    for idx, input_path in enumerate(tqdm(image_paths, desc=f\"Processing {bread_name}\")):\n",
    "        output_filename = f\"{idx:04d}.jpg\"\n",
    "        output_path = os.path.join(processed_dir, output_filename)\n",
    "        \n",
    "        success = process_image(input_path, output_path, target_size)\n",
    "        \n",
    "        if success:\n",
    "            successful_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            # Remove failed image if it was created\n",
    "            if os.path.exists(output_path):\n",
    "                os.remove(output_path)\n",
    "    \n",
    "    print(f\"Results for {bread_name}:\")\n",
    "    print(f\"  Successfully processed: {successful_count}\")\n",
    "    print(f\"  Failed/filtered out: {failed_count}\")\n",
    "    print(f\"  Final images saved to: {processed_dir}\")\n",
    "    \n",
    "    return successful_count\n",
    "\n",
    "# Apply preprocessing to all categories\n",
    "total_processed = 0\n",
    "\n",
    "for bread in bread_types:\n",
    "    safe_name = bread.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    image_dir = os.path.join(base_dir, safe_name, \"images\")\n",
    "    \n",
    "    if os.path.exists(image_dir):\n",
    "        count = preprocess_category_images(bread, image_dir)\n",
    "        total_processed += count\n",
    "    else:\n",
    "        print(f\"Warning: Directory not found for {bread}: {image_dir}\")\n",
    "\n",
    "print(f\"\\nAll image preprocessing completed.\")\n",
    "print(f\"Total images successfully processed: {total_processed}\")\n",
    "\n",
    "# Create summary report\n",
    "print(\"\\nSummary Report:\")\n",
    "print(\"-\" * 50)\n",
    "for bread in bread_types:\n",
    "    safe_name = bread.replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    processed_dir = os.path.join(base_dir, safe_name, \"processed\")\n",
    "    \n",
    "    if os.path.exists(processed_dir):\n",
    "        count = len(glob(os.path.join(processed_dir, \"*.jpg\")))\n",
    "        print(f\"{bread}: {count} processed images\")\n",
    "    else:\n",
    "        print(f\"{bread}: 0 processed images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
